<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta
      name="description"
      content=""
    />
    <meta name="author" content="Rahul Chand" />
    <meta name="theme-color" content="#222222" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="images/nicons/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="images/nicons/favicon.ico"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/nicons/favicon.ico"
    />

    <!-- Bootstrap CSS -->
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
      integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="style-3.css" />

    <title>Rahul Chand</title>
  </head>
  <body>
    <!-- <h1>Hello, world!</h1> -->
    <div class="container pt-5 allstuffp">
      <div class="col pt-5 allstuff">
        <div>
          <div style="font-size: 40px; text-decoration: underline;;">Papers I am reading/read</div>
        </div>
        <div class="pt-2 about">
            <div style="font-size: 18px;">Generic</div>
            <ul style="color: black;">
                <li>
                    Original MoE: <a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
                </a>
            </li>
                <li>How to train MoEs (vanilla): <a href="https://arxiv.org/abs/2101.03961">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</a></li>
            </ul>
        </div>
        <div class="pt-2 about">
            <div style="font-size: 18px;">Efficient LLM</div>
            <ul style="color: black;">
                <li><a href="https://arxiv.org/abs/2211.15841">MegaBlocks: Efficient Sparse Training with Mixture-of-Experts
                </a></li>
            </ul>
        </div>
        <div class="pt-2 about">
            <div style="font-size: 18px;">Alignment/Interpretability</div>
            <ul style="color: black;">
                <li>Best Neurips 2023 Paper (DPO): <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model
                </a></li>
                <li>RLHF: <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback
                </a></li>
                <li>
                    <a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions *
                    </a>
                </li>
            </ul>
        </div>
        <hr>
        * = To Be Read
      </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script
      src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
      integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
      integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
      crossorigin="anonymous"
    ></script>
    <script src="style-3.js"></script>
  </body>
  <style></style>
</html>
